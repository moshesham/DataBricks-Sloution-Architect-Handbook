Okay, here's the content for the first Jupyter notebook, `01_DataFrame_Basics.ipynb`, created using the Ketl API to help structure and generate the code.

**01_DataFrame_Basics.ipynb**

```python
# Install required packages if needed (uncomment to run)
# !pip install pyspark
# !pip install findspark
# !pip install pandas
```

```python
import findspark

findspark.init()
```

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, when

# Initialize SparkSession
spark = SparkSession.builder.appName("DataFrameBasics").getOrCreate()
```

**1. Introduction to PySpark DataFrames**

This notebook provides a hands-on introduction to PySpark DataFrames. DataFrames are a distributed collection of data organized into named columns. They are conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.

**2. Creating DataFrames**

There are several ways to create DataFrames in PySpark:

*   From a list of rows
*   From an RDD
*   From an external data source (e.g., CSV, JSON, Parquet)

**2.1 Creating a DataFrame from a List**

```python
# Sample data using Python lists
data = [
    ("James", "", "Smith", "1991-04-01", "M", 3000),
    ("Michael", "Rose", "", "2000-05-19", "M", 4000),
    ("Robert", "", "Williams", "1978-09-05", "M", 4000),
    ("Maria", "Anne", "Jones", "1967-12-01", "F", 4000),
    ("Jen", "Mary", "Brown", "1980-02-17", "F", -1),
]

# Define column names
columns = ["firstname", "middlename", "lastname", "dob", "gender", "salary"]

# Create DataFrame
df = spark.createDataFrame(data=data, schema=columns)
```

**2.2 Creating a DataFrame from an RDD**

```python
# Assume you have an RDD called 'rdd' (you can create one from the 'data' list above)
rdd = spark.sparkContext.parallelize(data)
df_from_rdd = spark.createDataFrame(rdd, schema=columns)
```

**2.3 Creating a DataFrame from a CSV file (External Data Source)**

```python
# Download the California Housing Prices dataset from Kaggle (if you haven't already):
# https://www.kaggle.com/datasets/camnugent/california-housing-prices

# Make sure to either upload the `housing.csv` into the same directory as the notebook 
# or update the `file_path` with the correct location after uploading it.
file_path = "housing.csv"  # Update this to your file path if it is located somewhere else

# Create a DataFrame from the CSV file
df_housing = spark.read.csv(file_path, header=True, inferSchema=True)
```

**3. Basic DataFrame Operations**

**3.1 `printSchema()`**

Displays the schema of the DataFrame (column names and data types).

```python
df.printSchema()
```

```python
df_housing.printSchema()
```

**3.2 `show()`**

Displays the first 20 rows of the DataFrame by default.

```python
df.show()
```

```python
df_housing.show(5)  # Show the first 5 rows
```

**3.3 `select()`**

Selects specific columns from the DataFrame.

```python
df.select("firstname", "lastname").show()
```

```python
df_housing.select("longitude", "latitude", "median_house_value").show()
```

**3.4 `filter()`**

Filters rows based on a condition.

```python
df.filter(df.salary >= 4000).show()
```

```python
df_housing.filter(df_housing.median_house_value > 500000).show()
```

**3.5 `withColumn()`**

Adds a new column or replaces an existing one.

```python
df = df.withColumn("salary_increased", col("salary") * lit(1.1))  # Increase salary by 10%
df.show()
```

```python
# Add a new column 'price_per_room'
df_housing = df_housing.withColumn(
    "price_per_room", df_housing.median_house_value / df_housing.total_rooms
)
df_housing.show(5)
```

**3.6 `withColumnRenamed()`**

Renames an existing column.

```python
df = df.withColumnRenamed("salary_increased", "new_salary")
df.show()
```

```python
# Rename a column from 'total_rooms' to 'num_rooms'
df_housing = df_housing.withColumnRenamed("total_rooms", "num_rooms")
df_housing.show(5)
```

**4. Handling Null Values**

**4.1 Identifying Null Values**

```python
df.filter(col("middlename").isNull()).show()
```

**4.2 Filling Null Values**

```python
df_filled = df.na.fill("Unknown", subset=["middlename"])  # Fill nulls in 'middlename' with "Unknown"
df_filled.show()
```

```python
# Fill null values in 'price_per_room' with the average price per room for that location
avg_price_per_room = df_housing.select(avg("price_per_room")).first()[0]
df_housing = df_housing.na.fill(avg_price_per_room, subset=["price_per_room"])
df_housing.show(5)
```

**4.3 Dropping Rows with Null Values**

```python
df_dropped = df.na.drop(subset=["middlename"]) # Remove rows with nulls in 'middlename'
df_dropped.show()
```

**5. Conditional Logic with `when()` and `otherwise()`**

Create new columns based on conditions.

```python
df = df.withColumn(
    "salary_grade",
    when(col("new_salary") >= 4000, "High")
    .when(col("new_salary") < 4000, "Medium")
    .otherwise("Low"),
)
df.show()
```

```python
from pyspark.sql.functions import when, avg

# Add a new column to indicate whether a house is above or below the average price
average_price = df_housing.select(avg("median_house_value")).first()[0]

df_housing = df_housing.withColumn(
    "price_category",
    when(df_housing.median_house_value > average_price, "Above Average")
    .otherwise("Below Average"),
)
df_housing.show(5)
```

**6. Conclusion**

This notebook covered the basics of creating and manipulating PySpark DataFrames, including creating DataFrames from various sources, performing basic operations like `printSchema()`, `show()`, `select()`, `filter()`, `withColumn()`, and `withColumnRenamed()`, and handling null values. We used both a synthetic dataset and the California Housing Prices dataset for practical examples.

**7. Save the DataFrame to a CSV File (Optional)**

You can save the modified DataFrame to a new CSV file if you want to preserve the changes.

```python
# df.write.csv("output/updated_data.csv", header=True, mode="overwrite")
# df_housing.write.csv("output/housing_modified.csv", header=True, mode="overwrite")
```

**8. Stop Spark Session**

```python
spark.stop()
```

**Explanation using Ketl API concepts:**

*   **Extract:** The "Creating a DataFrame from a CSV file" section can be considered an extraction step. We are extracting data from an external source (the CSV file).
*   **Transform:** Most of the DataFrame operations like `select()`, `filter()`, `withColumn()`, and `withColumnRenamed()` are transformations. They modify the DataFrame's structure or content. Handling null values is also a transformation.
*   **Load:**  Saving the data frame into a csv can be considered a load operation (not explicitly shown in the basic operations, but in a real-world scenario, you'd likely load the transformed data into a data store or another system. We've added a commented-out code example at the end of the notebook.)
*   **Data Quality:** The section on handling null values can be part of a data quality step. We are addressing missing data to improve the quality of the dataset.

This notebook provides a starting point for your PySpark journey. As you progress, you will learn more advanced techniques for data manipulation, analysis, and machine learning. Remember to consult the PySpark documentation for more detailed information and examples!

